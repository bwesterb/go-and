// Code generated by command: go run src.go -out ../../and_amd64.s -stubs ../../and_stubs_amd64.go -pkg and. DO NOT EDIT.

//go:build !purego

#include "textflag.h"

// func andAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPAND   Y1, Y0, Y1
	VPAND   Y3, Y2, Y3
	VPAND   Y5, Y4, Y5
	VPAND   Y7, Y6, Y7
	VPAND   Y9, Y8, Y9
	VPAND   Y11, Y10, Y11
	VPAND   Y13, Y12, Y13
	VPAND   Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func andAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·andAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPAND   X1, X0, X1
	VPAND   X3, X2, X3
	VPAND   X5, X4, X5
	VPAND   X7, X6, X7
	VPAND   X9, X8, X9
	VPAND   X11, X10, X11
	VPAND   X13, X12, X13
	VPAND   X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func orAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·orAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPOR    Y1, Y0, Y1
	VPOR    Y3, Y2, Y3
	VPOR    Y5, Y4, Y5
	VPOR    Y7, Y6, Y7
	VPOR    Y9, Y8, Y9
	VPOR    Y11, Y10, Y11
	VPOR    Y13, Y12, Y13
	VPOR    Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func orAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·orAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPOR    X1, X0, X1
	VPOR    X3, X2, X3
	VPOR    X5, X4, X5
	VPOR    X7, X6, X7
	VPOR    X9, X8, X9
	VPOR    X11, X10, X11
	VPOR    X13, X12, X13
	VPOR    X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func xorAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·xorAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPXOR   Y1, Y0, Y1
	VPXOR   Y3, Y2, Y3
	VPXOR   Y5, Y4, Y5
	VPXOR   Y7, Y6, Y7
	VPXOR   Y9, Y8, Y9
	VPXOR   Y11, Y10, Y11
	VPXOR   Y13, Y12, Y13
	VPXOR   Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func xorAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·xorAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPXOR   X1, X0, X1
	VPXOR   X3, X2, X3
	VPXOR   X5, X4, X5
	VPXOR   X7, X6, X7
	VPXOR   X9, X8, X9
	VPXOR   X11, X10, X11
	VPXOR   X13, X12, X13
	VPXOR   X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func andNotAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andNotAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPANDN  Y1, Y0, Y1
	VPANDN  Y3, Y2, Y3
	VPANDN  Y5, Y4, Y5
	VPANDN  Y7, Y6, Y7
	VPANDN  Y9, Y8, Y9
	VPANDN  Y11, Y10, Y11
	VPANDN  Y13, Y12, Y13
	VPANDN  Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func andNotAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·andNotAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPANDN  X1, X0, X1
	VPANDN  X3, X2, X3
	VPANDN  X5, X4, X5
	VPANDN  X7, X6, X7
	VPANDN  X9, X8, X9
	VPANDN  X11, X10, X11
	VPANDN  X13, X12, X13
	VPANDN  X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func popcntAsm(a *byte, l uint64) int
// Requires: POPCNT
TEXT ·popcntAsm(SB), NOSPLIT, $0-24
	MOVQ a+0(FP), AX
	MOVQ l+8(FP), CX
	XORQ DX, DX

loop:
	MOVQ    (AX), BX
	MOVQ    8(AX), SI
	MOVQ    16(AX), DI
	MOVQ    24(AX), R8
	MOVQ    32(AX), R9
	MOVQ    40(AX), R10
	MOVQ    48(AX), R11
	MOVQ    56(AX), R12
	POPCNTQ BX, BX
	POPCNTQ SI, SI
	POPCNTQ DI, DI
	POPCNTQ R8, R8
	POPCNTQ R9, R9
	POPCNTQ R10, R10
	POPCNTQ R11, R11
	POPCNTQ R12, R12
	ADDQ    BX, DX
	ADDQ    SI, DX
	ADDQ    DI, DX
	ADDQ    R8, DX
	ADDQ    R9, DX
	ADDQ    R10, DX
	ADDQ    R11, DX
	ADDQ    R12, DX
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	MOVQ    DX, ret+16(FP)
	RET

// func memsetAVX2(dst *byte, l uint64, b byte)
// Requires: AVX, AVX2
TEXT ·memsetAVX2(SB), NOSPLIT, $0-17
	MOVQ         dst+0(FP), AX
	MOVQ         l+8(FP), CX
	VPBROADCASTB b+16(FP), Y0

loop:
	VMOVDQU Y0, (AX)
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	VZEROALL
	RET

// func memsetAVX(dst *byte, l uint64, b byte)
// Requires: AVX, SSE2
TEXT ·memsetAVX(SB), NOSPLIT, $0-17
	MOVQ    dst+0(FP), AX
	MOVQ    l+8(FP), CX
	MOVB    b+16(FP), DL
	MOVQ    DX, X0
	VPSHUFB zeroes<>+0(SB), X0, X0

loop:
	VMOVDQU X0, (AX)
	ADDQ    $0x00000010, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	VZEROALL
	RET

DATA zeroes<>+0(SB)/4, $0x00000000
DATA zeroes<>+4(SB)/4, $0x00000000
DATA zeroes<>+8(SB)/4, $0x00000000
DATA zeroes<>+12(SB)/4, $0x00000000
GLOBL zeroes<>(SB), RODATA|NOPTR, $16
