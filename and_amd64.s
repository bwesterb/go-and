// Code generated by command: go run src.go -out ../../and_amd64.s -stubs ../../and_stubs_amd64.go -pkg and. DO NOT EDIT.

//go:build !purego

#include "textflag.h"

// func andAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPAND   Y1, Y0, Y1
	VPAND   Y3, Y2, Y3
	VPAND   Y5, Y4, Y5
	VPAND   Y7, Y6, Y7
	VPAND   Y9, Y8, Y9
	VPAND   Y11, Y10, Y11
	VPAND   Y13, Y12, Y13
	VPAND   Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func andAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·andAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPAND   X1, X0, X1
	VPAND   X3, X2, X3
	VPAND   X5, X4, X5
	VPAND   X7, X6, X7
	VPAND   X9, X8, X9
	VPAND   X11, X10, X11
	VPAND   X13, X12, X13
	VPAND   X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func orAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·orAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPOR    Y1, Y0, Y1
	VPOR    Y3, Y2, Y3
	VPOR    Y5, Y4, Y5
	VPOR    Y7, Y6, Y7
	VPOR    Y9, Y8, Y9
	VPOR    Y11, Y10, Y11
	VPOR    Y13, Y12, Y13
	VPOR    Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func orAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·orAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPOR    X1, X0, X1
	VPOR    X3, X2, X3
	VPOR    X5, X4, X5
	VPOR    X7, X6, X7
	VPOR    X9, X8, X9
	VPOR    X11, X10, X11
	VPOR    X13, X12, X13
	VPOR    X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func xorAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·xorAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPXOR   Y1, Y0, Y1
	VPXOR   Y3, Y2, Y3
	VPXOR   Y5, Y4, Y5
	VPXOR   Y7, Y6, Y7
	VPXOR   Y9, Y8, Y9
	VPXOR   Y11, Y10, Y11
	VPXOR   Y13, Y12, Y13
	VPXOR   Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func xorAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·xorAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPXOR   X1, X0, X1
	VPXOR   X3, X2, X3
	VPXOR   X5, X4, X5
	VPXOR   X7, X6, X7
	VPXOR   X9, X8, X9
	VPXOR   X11, X10, X11
	VPXOR   X13, X12, X13
	VPXOR   X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func andNotAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andNotAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPANDN  Y1, Y0, Y1
	VPANDN  Y3, Y2, Y3
	VPANDN  Y5, Y4, Y5
	VPANDN  Y7, Y6, Y7
	VPANDN  Y9, Y8, Y9
	VPANDN  Y11, Y10, Y11
	VPANDN  Y13, Y12, Y13
	VPANDN  Y15, Y14, Y15
	VMOVDQU Y1, (DX)
	VMOVDQU Y3, 32(DX)
	VMOVDQU Y5, 64(DX)
	VMOVDQU Y7, 96(DX)
	VMOVDQU Y9, 128(DX)
	VMOVDQU Y11, 160(DX)
	VMOVDQU Y13, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func andNotAVX(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX
TEXT ·andNotAVX(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPANDN  X1, X0, X1
	VPANDN  X3, X2, X3
	VPANDN  X5, X4, X5
	VPANDN  X7, X6, X7
	VPANDN  X9, X8, X9
	VPANDN  X11, X10, X11
	VPANDN  X13, X12, X13
	VPANDN  X15, X14, X15
	VMOVDQU X1, (DX)
	VMOVDQU X3, 16(DX)
	VMOVDQU X5, 32(DX)
	VMOVDQU X7, 48(DX)
	VMOVDQU X9, 64(DX)
	VMOVDQU X11, 80(DX)
	VMOVDQU X13, 96(DX)
	VMOVDQU X15, 112(DX)
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	VZEROALL
	RET

// func notAVX2(dst *byte, a *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·notAVX2(SB), NOSPLIT, $0-24
	MOVQ dst+0(FP), AX
	MOVQ a+8(FP), CX
	MOVQ l+16(FP), DX

	// Initialize this register to all ones, so we can XOR with it to simulate a NOT
	VPCMPEQB Y0, Y0, Y0

loop:
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5
	VMOVDQU 160(CX), Y6
	VMOVDQU 192(CX), Y7
	VMOVDQU 224(CX), Y8
	VPXOR   Y1, Y0, Y1
	VPXOR   Y2, Y0, Y2
	VPXOR   Y3, Y0, Y3
	VPXOR   Y4, Y0, Y4
	VPXOR   Y5, Y0, Y5
	VPXOR   Y6, Y0, Y6
	VPXOR   Y7, Y0, Y7
	VPXOR   Y8, Y0, Y8
	VMOVDQU Y1, (AX)
	VMOVDQU Y2, 32(AX)
	VMOVDQU Y3, 64(AX)
	VMOVDQU Y4, 96(AX)
	VMOVDQU Y5, 128(AX)
	VMOVDQU Y6, 160(AX)
	VMOVDQU Y7, 192(AX)
	VMOVDQU Y8, 224(AX)
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, AX
	SUBQ    $0x00000001, DX
	JNZ     loop
	VZEROALL
	RET

// func notAVX(dst *byte, a *byte, l uint64)
// Requires: AVX
TEXT ·notAVX(SB), NOSPLIT, $0-24
	MOVQ dst+0(FP), AX
	MOVQ a+8(FP), CX
	MOVQ l+16(FP), DX

	// Initialize this register to all ones, so we can XOR with it to simulate a NOT
	VPCMPEQB X0, X0, X0

loop:
	VMOVDQU (CX), X1
	VMOVDQU 16(CX), X2
	VMOVDQU 32(CX), X3
	VMOVDQU 48(CX), X4
	VMOVDQU 64(CX), X5
	VMOVDQU 80(CX), X6
	VMOVDQU 96(CX), X7
	VMOVDQU 112(CX), X8
	VPXOR   X1, X0, X1
	VPXOR   X2, X0, X2
	VPXOR   X3, X0, X3
	VPXOR   X4, X0, X4
	VPXOR   X5, X0, X5
	VPXOR   X6, X0, X6
	VPXOR   X7, X0, X7
	VPXOR   X8, X0, X8
	VMOVDQU X1, (AX)
	VMOVDQU X2, 16(AX)
	VMOVDQU X3, 32(AX)
	VMOVDQU X4, 48(AX)
	VMOVDQU X5, 64(AX)
	VMOVDQU X6, 80(AX)
	VMOVDQU X7, 96(AX)
	VMOVDQU X8, 112(AX)
	ADDQ    $0x00000080, CX
	ADDQ    $0x00000080, AX
	SUBQ    $0x00000001, DX
	JNZ     loop
	VZEROALL
	RET

// func popcntAsm(a *byte, l uint64) int
// Requires: POPCNT
TEXT ·popcntAsm(SB), NOSPLIT, $0-24
	MOVQ a+0(FP), AX
	MOVQ l+8(FP), CX
	XORQ DX, DX

loop:
	MOVQ    (AX), BX
	MOVQ    8(AX), SI
	MOVQ    16(AX), DI
	MOVQ    24(AX), R8
	MOVQ    32(AX), R9
	MOVQ    40(AX), R10
	MOVQ    48(AX), R11
	MOVQ    56(AX), R12
	POPCNTQ BX, BX
	POPCNTQ SI, SI
	POPCNTQ DI, DI
	POPCNTQ R8, R8
	POPCNTQ R9, R9
	POPCNTQ R10, R10
	POPCNTQ R11, R11
	POPCNTQ R12, R12
	ADDQ    BX, DX
	ADDQ    SI, DX
	ADDQ    DI, DX
	ADDQ    R8, DX
	ADDQ    R9, DX
	ADDQ    R10, DX
	ADDQ    R11, DX
	ADDQ    R12, DX
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	MOVQ    DX, ret+16(FP)
	RET

// func memsetAVX2(dst *byte, l uint64, b byte)
// Requires: AVX, AVX2
TEXT ·memsetAVX2(SB), NOSPLIT, $0-17
	MOVQ         dst+0(FP), AX
	MOVQ         l+8(FP), CX
	VPBROADCASTB b+16(FP), Y0

loop:
	VMOVDQU Y0, (AX)
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	VZEROALL
	RET

// func memsetAVX(dst *byte, l uint64, b byte)
// Requires: AVX, SSE2
TEXT ·memsetAVX(SB), NOSPLIT, $0-17
	MOVQ    dst+0(FP), AX
	MOVQ    l+8(FP), CX
	MOVB    b+16(FP), DL
	MOVQ    DX, X0
	VPSHUFB zeroes<>+0(SB), X0, X0

loop:
	VMOVDQU X0, (AX)
	ADDQ    $0x00000010, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	VZEROALL
	RET

DATA zeroes<>+0(SB)/4, $0x00000000
DATA zeroes<>+4(SB)/4, $0x00000000
DATA zeroes<>+8(SB)/4, $0x00000000
DATA zeroes<>+12(SB)/4, $0x00000000
GLOBL zeroes<>(SB), RODATA|NOPTR, $16

// func anyMaskedAVX2(a *byte, b *byte, l uint64) bool
// Requires: AVX
TEXT ·anyMaskedAVX2(SB), NOSPLIT, $0-25
	MOVQ a+0(FP), AX
	MOVQ b+8(FP), CX
	MOVQ l+16(FP), DX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(AX), Y2
	VMOVDQU 32(CX), Y3
	VMOVDQU 64(AX), Y4
	VMOVDQU 64(CX), Y5
	VMOVDQU 96(AX), Y6
	VMOVDQU 96(CX), Y7
	VMOVDQU 128(AX), Y8
	VMOVDQU 128(CX), Y9
	VMOVDQU 160(AX), Y10
	VMOVDQU 160(CX), Y11
	VMOVDQU 192(AX), Y12
	VMOVDQU 192(CX), Y13
	VMOVDQU 224(AX), Y14
	VMOVDQU 224(CX), Y15
	VPTEST  Y0, Y1
	JNZ     found
	VPTEST  Y2, Y3
	JNZ     found
	VPTEST  Y4, Y5
	JNZ     found
	VPTEST  Y6, Y7
	JNZ     found
	VPTEST  Y8, Y9
	JNZ     found
	VPTEST  Y10, Y11
	JNZ     found
	VPTEST  Y12, Y13
	JNZ     found
	VPTEST  Y14, Y15
	JNZ     found
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	SUBQ    $0x00000001, DX
	JNZ     loop
	MOVL    $0x00000000, AX
	VZEROALL
	RET

found:
	MOVL $0x00000001, AX
	VZEROALL
	RET

// func anyMaskedAVX(a *byte, b *byte, l uint64) bool
// Requires: AVX
TEXT ·anyMaskedAVX(SB), NOSPLIT, $0-25
	MOVQ a+0(FP), AX
	MOVQ b+8(FP), CX
	MOVQ l+16(FP), DX

loop:
	VMOVDQU (AX), X0
	VMOVDQU (CX), X1
	VMOVDQU 16(AX), X2
	VMOVDQU 16(CX), X3
	VMOVDQU 32(AX), X4
	VMOVDQU 32(CX), X5
	VMOVDQU 48(AX), X6
	VMOVDQU 48(CX), X7
	VMOVDQU 64(AX), X8
	VMOVDQU 64(CX), X9
	VMOVDQU 80(AX), X10
	VMOVDQU 80(CX), X11
	VMOVDQU 96(AX), X12
	VMOVDQU 96(CX), X13
	VMOVDQU 112(AX), X14
	VMOVDQU 112(CX), X15
	VPTEST  X0, X1
	JNZ     found
	VPTEST  X2, X3
	JNZ     found
	VPTEST  X4, X5
	JNZ     found
	VPTEST  X6, X7
	JNZ     found
	VPTEST  X8, X9
	JNZ     found
	VPTEST  X10, X11
	JNZ     found
	VPTEST  X12, X13
	JNZ     found
	VPTEST  X14, X15
	JNZ     found
	ADDQ    $0x00000080, AX
	ADDQ    $0x00000080, CX
	SUBQ    $0x00000001, DX
	JNZ     loop
	MOVL    $0x00000000, AX
	VZEROALL
	RET

found:
	MOVL $0x00000001, AX
	VZEROALL
	RET
